<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>kidlovec.github.io</title>
   
   <link>http://localhost:4000</link>
   <description>做一个谦卑，忠诚的实践者，不要做一个严谨，深沉的思想家. The story begins here.</description>
   <language>en-uk</language>
   <managingEditor> </managingEditor>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>Flink Introduction</title>
	  <link>/2018-05-04-flink-introduce/</link>
	  <author></author>
	  <pubDate>2018-05-04T18:18:00+08:00</pubDate>
	  <guid>/2018-05-04-flink-introduce/</guid>
	  <description><![CDATA[
	     <p><strong>目录</strong></p>

<h1 id="0--what-is-apache-flink">0.  What is Apache Flink</h1>

<blockquote>
  <p>是一个支持实时分析和实时计算的开源的分布式 <strong>流处理</strong> 平台。 Apache Flink 能够提供高效，快速，准确和容错的高容量的有界/无界事件流处理能力。</p>
</blockquote>

<p>Apache Flink 是一个框架和分布式处理引擎，用于在无边界和有边界数据流上进行有状态的计算。Flink 能在所有常见集群环境中运行，并能以内存速度和任意规模进行计算。</p>

<h2 id="处理无界和有界数据">处理无界和有界数据</h2>
<p>任何类型的数据都可以形成一种事件流。信用卡交易、传感器测量、机器日志、网站或移动应用程序上的用户交互记录，所有这些数据都形成一种流。</p>

<p><img src="./../assets/img/bounded-unbounded.png" alt="" /></p>

<p>数据可以被作为 无界 或者 有界 流来处理。</p>

<p>无界流 有定义流的开始，但没有定义流的结束。</p>

<blockquote>
  <p>它们会无休止地产生数据。无界流的数据必须持续处理，即数据被摄取后需要立刻处理。</p>
</blockquote>

<p>我们不能等到所有数据都到达再处理，因为输入是无限的，在任何时候输入都不会完成。
处理无界数据通常要求以特定顺序摄取事件，例如事件发生的顺序，以便能够推断结果的完整性。</p>

<p>有界流 有定义流的开始，也有定义流的结束。有界流可以在摄取所有数据后再进行计算。有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理</p>

<h1 id="1-前世今生">1. 前世今生</h1>

<p>Flink 起源于 2009 年，柏林工业大学的一个名叫 <strong><a href="http://stratosphere.eu/">Stratosphere</a> 「平流层」</strong> 的研究项目。旨在 <strong>设计下一代的大数据分析平台</strong> 。</p>
<ul>
  <li>在 2014 年 4月贡献给了 Apache 基金会。
在 2015 年 1月成为 Apache 顶级项目</li>
</ul>

<blockquote>
  <p>初代的 Stratosphere 专注于 底层的运行时，优化器，和 java API的设计。而后，整个平台趋于 成熟, 0.6 之后 更名 Flink 。0.7 之后 引入了 Streaming API 。</p>
</blockquote>

<h2 id="1-早期的设计">1. 早期的设计</h2>

<p><img src="./../assets/img/the-origin-design-1.png" alt="" /></p>

<h2 id="2-早期的设计变迁">2. 早期的设计变迁</h2>

<p><img src="./../assets/img/the-evolution-of-Stratosphere-happened-over-time.png" alt="" /></p>

<h2 id="3-优点特性">3. 优点/特性</h2>

<ul>
  <li>
    <p>高性能.
Flink旨在实现高性能和低延迟。与其他流式框架（如Spark）不同，您无需执行许多手动配置即可获得最佳性能。与其对应物相比，Flink的流水线数据处理提供了更好的性能。</p>
  </li>
  <li>
    <p>Exactly-once 状态计算
Flink的分布式检查点处理有助于保证每个记录只处理一次。在高吞吐量应用程序的情况下，Flink为我们提供了一个允许至少一次处理的开关。</p>
  </li>
  <li>
    <p>灵活的流式传输windows
Flink支持数据驱动的窗口。这意味着我们可以根据时间，计数或会话设计一个窗口。还可以定制一个窗口，允许我们检测事件流中的特定模式.</p>
  </li>
  <li>
    <p>Fault tolerance
Flink的分布式轻量级快照机制有助于实现高度的容错能力。它允许Flink提供高吞吐量的性能和有保证的交付。内存管理.Flink在JVM中提供了自己的内存管理，使其独立于Java的默认垃圾收集器。它通过使用散列，索引，缓存和排序有效地进行内存管理.</p>
  </li>
  <li>
    <p>Optimizer
Flink的批处理数据处理 API 经过优化，以避免消耗内存，排序等内存消耗。它还确保使用缓存以避免繁重的磁盘IO操作。</p>
  </li>
  <li>
    <p>platform 一个统一的计算平台</p>
  </li>
</ul>

<p>Flink中的流和批处理为批处理和流数据处理提供 API 。因此，一旦设置了Flink环境，它就可以轻松地托管流和批处理应用程序。事实上，Flink在Streaming第一原理上工作，并将批处理视为streaming的特殊情况.</p>

<ul>
  <li>
    <p>Libraries
Flink拥有丰富的库来进行机器学习，图形处理，关系数据处理等。由于其架构，很容易执行复杂的事件处理和警报。</p>
  </li>
  <li>
    <p>事件时间语义.
Flink支持事件时间语义。这有助于处理事件无序到达的流。有时事件可能会延迟。 Flink的架构允许我们根据时间，计数和会话定义窗口，这有助于处理这些场景。</p>
  </li>
</ul>

<h2 id="4--应用场景">4. <a name=""></a> 应用场景</h2>

<ul>
  <li>Event-Driven Applications
    <ul>
      <li>实时推荐系统</li>
      <li>模式识别类的应用（如 信用卡交易中的欺诈检测）</li>
      <li>支付宝安全检测</li>
    </ul>
  </li>
  <li>Data Pipelines
    <ul>
      <li>系统升级，数据迁移等</li>
    </ul>
  </li>
  <li>Streaming Analytics</li>
</ul>

<hr />

<h1 id="概念介绍">概念介绍</h1>

<ul>
  <li>What is Stream Processing?</li>
</ul>

<blockquote>
  <p>事件驱动型应用是一类具有状态的应用，它从一个或多个事件流提取数据，并根据到来的事件触发计算、状态更新或其他外部动作。</p>
</blockquote>

<blockquote>
  <p>事件驱动型应用是在计算存储分离的传统应用基础上进化而来。在传统架构中，应用需要读写远程事务型数据库。</p>
</blockquote>

<blockquote>
  <p>相反，事件驱动型应用是基于状态化流处理来完成。在该设计中，数据和计算不会分离，应用只需访问本地（内存或磁盘）即可获取数据。系统容错性的实现依赖于定期向远程持久化存储写入 checkpoint。下图描述了传统应用和事件驱动型应用架构的区别。</p>
</blockquote>

<p><img src="./../assets/img/stream-vs-tradition.jpg" alt="" /></p>

<blockquote>
  <p>例如基于事件时间的有且仅有一次性语义保证和数据窗口，开发人员不再需要在应用程序层中考虑这些问题。 这种风格导致更少的错误。</p>
</blockquote>

<blockquote>
  <p>原有的开源大数据计算方案(lambda 架构)，会把流处理和批处理作为两种不同的应用类型，因为它们所提供的SLA（Service-Level-Aggreement）是完全不相同的：流处理一般需要支持低延迟、Exactly-once保证，而批处理需要支持高吞吐、高效处理。</p>
</blockquote>

<p><img src="./../assets/img/lambda.png" alt="" /></p>

<blockquote>
  <p>Flink从另一个视角看待流处理和批处理，将二者统一起来：Flink是完全支持流处理，也就是说作为流处理看待时输入数据流是无界的；<strong>批处理被作为一种特殊的流处理，只是它的输入数据流被定义为有界的</strong>。</p>
</blockquote>

<h2 id="3-数据流编程模型">3. 数据流编程模型</h2>

<h3 id="31-抽象级别">3.1. 抽象级别</h3>

<ul>
  <li>Flink提供了不同级别的抽象，以开发流或批处理作业。</li>
</ul>

<p><img src="./../assets/img/levels_of_abstraction.svg" alt="" /></p>

<hr />

<ul>
  <li>最底层级的抽象仅仅提供了有状态流。</li>
</ul>

<blockquote>
  <p>它将通过过程函数（Process Function）嵌入到 DataStream API 中。它允许用户可以自由地处理来自一个或多个数据流的件，并使用一致的容错的状态。除此之外，用户可以注册事件时间并处理时间回调，从使程序可以处理复杂的计算。</p>
</blockquote>

<ul>
  <li>实际上，大多数应用并不需要上述的底层抽象，而是针对 核心 API （Core API ） 进行编程</li>
</ul>

<blockquote>
  <p>比如 DataStream API （有界或无界流数据）以及 DataSet API 有界数据集）。这些 API 为数据处理提供了通用的构建模块，比如由用户定义的多种式的转换（transformations），连接（joins），聚合（aggregations），窗操作（windows），状态（state）等等。这些 API 处理的数据类型以类（classe）的形式由各自的编程语言所表示。</p>
</blockquote>

<ul>
  <li>
    <p>底层 过程函数（Process Function） 与 DataStream API相集成，使其可以某些特定的操作进行底层的抽象。</p>

    <p>DataSet API为有界数据集提供了额外的原语，如循环与迭代。</p>
  </li>
  <li>Table API是以 表 为中心的声明式DSL，其中表可能会动态变化（在表达流数据）。
Table API 遵循（扩展的）关系模型：表有二维数据结构（schema）（类似于系数据库中的表），同时 API 提供可比较的操作，例如select、project、joi、group-by、aggregate等。Table API 程序声明式地定义了 什么逻辑操作应该行 而不是准确地确定 这些操作代码的看上去如何 。 尽管Table API 可以通过多类型的用户自定义函数（UDF）进行扩展，其仍不如 核心 API更具表达能力，但是使起来却更加简洁（代码量更少）。除此之外，Table API 程序在执行之前会经过内置化器进行优化。</li>
  <li>你可以在表与 DataStream / DataSet 之间无缝切换，以允许程序将 TableAPI与 DataStream 以及 DataSet 混合使用。</li>
</ul>

<blockquote>
  <p>用户可以通过各种方法将数据进行转换 / 计算。
    - map
    - flatmap
    - window
    - keyby
    - sum
    - max
    - min
    - avg
    - join 等</p>
</blockquote>

<ul>
  <li>Flink提供的最高层级的抽象是 SQL 。这一层抽象在语法与表达能力上与 TableAPI类似，但是是以SQL查询表达式的形式表现程序。SQL抽象与Table API 交互密，同时SQL查询可以直接在Table API 定义的表上执行。</li>
</ul>

<h3 id="32-程序与数据流">3.2. <a name="-1"></a>程序与数据流</h3>

<ul>
  <li>
    <p>Flink程序的基础构建模块是 流（streams） 与 转换（transformations）。（需要注意的是，Flink的 DataSet API 所使用的 DataSet s其内部也是流。）概念上来讲，流是（可能永无止境的）数据记录流，而 转换 是一种操作，它取一个或多个流作为输入，并生产出一个或多个输出流作为结果。</p>
  </li>
  <li>
    <p>执行时，Flink程序映射到 流数据流（streaming dataflows） ，由 流 以及转换 算子 构成。每一个数据流起始于一个或多个 source，并终止于一个或多个 sink。数据流类似于任意的 有向无环图 （DAG） 。虽然通过 迭代 构造允许特定形式的环，但是大多数情况下，简单起见，我们都不考虑这一点。</p>
  </li>
</ul>

<p><img src="./../assets/img/program_dataflow.svg" alt="" /></p>

<p>通常，程序中的转换与数据流中的操作之间是一对一的关系。有时，然而，一个转换可能由多个转换操作构成。</p>

<ol>
  <li>
    <p>Source: 数据源，Flink 在流处理和批处理上的 source 大概有 4 类：基于本地集合的 source、基于文件的 source、基于网络套接字的 source、自定义的 source。自定义的 source 常见的有 Apache kafka、Amazon Kinesis Streams、RabbitMQ、Twitter Streaming API 、Apache NiFi 等，当然你也可以定义自己的 source。</p>
  </li>
  <li>
    <p>Transformation：数据转换的各种操作，有 Map / FlatMap / Filter / KeyBy / Reduce / Fold / Aggregations / Window / WindowAll / Union / Window join / Split / Select / Project 等，操作很多，可以将数据转换计算成你想要的数据。</p>
  </li>
  <li>
    <p>Sink：接收器，Flink 将转换计算后的数据发送的地点 ，你可能需要存储下来，Flink 常见的 Sink 大概有如下几类：写入文件、打印出来、写入 socket 、自定义的 sink 。自定义的 sink 常见的有 Apache kafka、RabbitMQ、MySQL、ElasticSearch、Apache Cassandra、Hadoop FileSystem 等，同理你也可以定义自己的 sink。</p>
  </li>
</ol>

<h3 id="33-并行数据流">3.3. <a name="-1"></a>并行数据流</h3>

<p>Flink程序本质上是并行分布的。在执行过程中，一个 流 包含一个或多个 流分区 ，而每一个 算子 包含一个或多个 算子 子任务 。操作子任务间彼此独立，以不同的线程执行，甚至有可能运行在不同的机器或容器上。</p>

<p>算子 子任务的数量即这一特定 算子 的 <strong>并行度</strong> 。一个流的并行度即其生产算子的并行度。相同程序中的不同的算子可能有不同级别的并行度。</p>

<p><img src="./../assets/img/parallel_dataflow.svg" alt="" /></p>

<p>流在两个算子之间传输数据，可以通过 一对一 （或称 forwarding ）模式，或者通过 redistributing 模式：</p>

<ul>
  <li>一对一 流
    <blockquote>
      <p>（例如上图中 Source 与 map算子之间）保持了元素的分区与排序。那意味着 map 算子的子任务[1]将以与 Source 的子任务[1]生成顺序相同的顺序查看到相同的元素。</p>
    </blockquote>
  </li>
  <li>Redistributing 流（如上图中 map() 与 keyBy/window 之间，以及 keyBy/window 与 Sink 之间）则改变了流的分区。
    <blockquote>
      <p>每一个算子子任务 根据所选择的转换，向不同的目标子任务发送数据。
比如 <code>keyBy()</code> （根据key的哈希值重新分区）， broadcast() ，或者 rebalance() （随机重分区）。
在一次 redistributing 交换中，元素间的排序只保留在每对发送与接受子任务中（比如， map() 的子任务[1]与 keyBy/window 的子任务[2]）。
因此在这个例子中，每个键的顺序被保留下来，但是并行确实引入了对于不同键的聚合结果到达sink的顺序的不确定性。</p>
    </blockquote>
  </li>
</ul>

<hr />

<h3 id="34-窗口-window">3.4. <a name="window"></a>窗口 window</h3>

<p>聚合事件（比如计数、求和）在流上的工作方式与批处理不同。比如，对流中的所有元素进行计数是不可能的，因为通常流是无限的（无界的）。相反，流上的聚合需要由 窗口 来划定范围，比如 “计算过去的5分钟” ，或者 “最后100个元素的和” 。
<img src="./../assets/img/windows.svg" alt="" /></p>

<p>窗口可以是 事件驱动的 （比如：每30秒）或者 数据驱动的 （比如：每100个元素）。窗口通常被区分为不同的类型，比如 滚动窗口 （没有重叠）， 滑动窗口 （有重叠），以及 会话窗口 （由不活动的间隙所打断）</p>

<p><img src="./../assets/img/window-types.png" alt="" /></p>

<h3 id="35-时间-time">3.5. <a name="time"></a>时间 time</h3>

<p>当提到流程序（例如定义窗口）中的时间时，你可以参考不同的时间概念：
<img src="./../assets/img/time-types.png" alt="" /></p>

<ul>
  <li>
    <p>事件时间(Event Time) 是事件创建的时间。它通常由事件中的时间戳描述，例如附接在生产传感器，或者生产服务。Flink通过时间戳分配器访问事件时间戳。</p>
  </li>
  <li>
    <p>摄入时间(Ingestion Time) 是事件进入Flink数据流源算子的时间。</p>
  </li>
  <li>
    <p>处理事件(Processing Time) 是每一个执行时间操作的算子的本地时间。</p>
  </li>
</ul>

<h3 id="36-有状态操作-statefual-operate">3.6. <a name="statefualoperate"></a>有状态操作 statefual operate</h3>

<p>尽管数据流中的很多操作一次只查看一个独立的事件（比如事件解析器），有些操作却会记录多个事件间的信息（比如窗口算子）。 这些操作被称为 有状态的 。</p>

<p>有状态操作的状态保存在一个可被视作嵌入式键/值存储的部分中。状态与由有状态算子读取的流一起被严格地分区与分布。因此，只能访问一个 keyBy() 函数之后的 keyed streams 的键/值状态，并且仅限于与当前事件键相关联的值。对齐流和状态的键确保了所有状态更新都是本地操作，以在没有事务开销的情况下确保一致性。这种对齐还使得Flink可以透明地重新分配状态与调整流的分区。</p>

<p><img src="./../assets/img/state_partitioning.svg" alt="" /></p>

<h3 id="37-容错检查点-checkpoint">3.7. <a name="checkpoint"></a>容错检查点 checkpoint</h3>

<p>Flink使用 流重放 与 Checkpoint 的结合实现了容错。Checkpoint与每一个输入流及其相关的每一个算子的状态的特定点相关联。一个流数据流可以可以从一个checkpoint恢复出来，其中通过恢复算子状态并从检查点重放事件以保持一致性（一次处理语义）</p>

<p>检查点间隔是以恢复时间（需要重放的事件数量）来消除执行过程中容错的开销的一种手段。</p>

<blockquote>
  <p>它不断为 分布式数据流 和 执行器状态 拍摄快照. 设计灵感来自 Chandy-Lamport 算法，但已根据Flink的定制要求进行了修改,如果发生任何故障，Flink将停止执行程序并重置它们并从最新的可用检查点开始执行,在绘制快照时，Flink处理记录的对齐，以避免因任何故障而重新处理相同的记录</p>
</blockquote>

<h3 id="38-流上的批处理">3.8. <a name="-1"></a>流上的批处理</h3>

<p>Flink将批处理程序作为流处理程序的特殊情况来执行，只是流是有界的（有限个元素）。 DataSet 内部被视为数据流。上述适用于流处理程序的概念同样适用于批处理程序，除了一些例外：</p>

<ul>
  <li>
    <p>DataSet API 中的程序不使用检查点。而通过完全地重放流来恢复。因为输入是有界的，因此这是可行的。这种方法使得恢复的成本增加，但是由于避免了检查点，因而使得正常处理的开销更小。</p>
  </li>
  <li>
    <p>DataSet API 中的有状态操作使用简化的im-memory/out-of-core数据结构，而不是键/值索引。</p>
  </li>
  <li>
    <p>DataSet API 引入了特殊的同步（基于superstep的）迭代，而这种迭代仅仅能在有界流上执行。细节可以查看迭代文档。</p>
  </li>
</ul>

<hr />

<h2 id="4-分布式运行时环境">4. <a name="-1"></a>分布式运行时环境</h2>

<h3 id="41-任务和算子链">4.1. <a name="-1"></a>任务和算子链</h3>

<p>分布式计算中，Flink会将算子（operator） 的子task链式组成tasks，每个task由一个线程执行。把算子链化为tasks是一个非常好的优化：它减少了线程之间的通信和缓冲，而且还能增加吞吐量降低延迟。</p>

<p>下图中dataflow有5个subtasks，因此有5个线程并发进行处理。</p>

<p><img src="./../assets/img/tasks_chains.svg" alt="" /></p>

<h3 id="42-job-managers-task-managers-clients">4.2. <a name="JobManagersTaskManagersClients"></a>Job Managers, Task Managers, Clients</h3>

<p>Flink运行时包含两类进程：</p>

<ul>
  <li>
    <p>JobManagers （也称为 masters）用来协调分布式计算。负责进行任务调度，协调checkpoints，协调错误恢复等等。至少需要一个JobManager。高可用部署下会有多个JobManagers，其中一个作为leader，其余处于standby状态。</p>
  </li>
  <li>
    <p>TaskManagers（也称为 workers）真正执行dataflow中的tasks（更准确的描述是，subtasks），并且对 streams进行缓存和交换。至少需要一个TaskManager。</p>
  </li>
</ul>

<p>有多种方式可以启动JobManagers和TaskManagers：直接在计算机上启动作为 standalone cluster，在容器中或者由资源管理器YARN 或者 Mesos启动。 TaskManagers连接到JobManagers后，会通知JobManagers自己已可用，接着被分配工作。</p>

<p>client 不作为运行时（runtime）和程序执行的一部分，只是用于准备和发送dataflow作业给JobManager。 因此客户端可以断开连接，或者保持连接以接收进度报告。客户端可以作为触发执行的Java/Scala 程序的一部分或者运行在命令行进程中 <code>./bin/flink run ...</code>。</p>

<p><img src="./../assets/img/processes.svg" alt="" /></p>

<h3 id="43-task-slots-and-resources">4.3. <a name="TaskSlotsandResources"></a>Task Slots and Resources</h3>

<blockquote>
  <p>‘Flink数据流默认是以分布式并行的方式执行的。对于并行数据处理，Flink对 算子和流进行分区。算子 分区称为子任务(sub tasks)。’</p>
</blockquote>

<p>每个 worker ( TaskManager)都是一个 JVM 进程，并且可以在不同的线程中执行一个或多个subtasks。每个 worker用 task slots（任务槽位） (至少有一个)来控制可以接收多少个 tasks。</p>

<p>每个task slot代表TaskManager中一个固定的资源子集。例如，有3个slots的TaskManager会将它的内存资源划分成3份分配给每个slot。划分资源意味着subtask不会和来自其他作业的subtasks竞争资源，但是也意味着它只拥有固定的内存资源。注意划分资源不进行CPU隔离，只划分内存资源给不同的tasks。</p>

<p>通过调整 slots 的个数进而可以调整 subtasks 之间的隔离方式。当每个 TaskManager 只有一个 slot 时，意味着每个 task group 运行在不同的JVM中（例如：可能在不同的container中）。当每个TaskManager有多个slots时，意味着多个subtasks可以共享同一个JVM。同一个JVM中的tasks共享TCP连接（通过多路复用技术）和心跳消息。可能还会共享数据集和数据结构，从而减少每个task的开销。</p>

<p><img src="./../assets/img/tasks_slots.svg" alt="" /></p>

<p>默认情况下，只要subtasks是来自同一个job，Flink允许不同tasks的subtasks共享slots。因此，一个slot可能会负责job的整个pipeline。允许slot sharing有两个好处：</p>

<p>Flink集群需要的slots的数量和job的最高并发度相同，不需要计算一个作业总共包含多少个tasks（具有不同并行度）。</p>

<p>更易获取更好的资源利用率。没有slot sharing，非集中型subtasks（source/map()）将会占用和集中型subtasks （window）一样多的资源。在我们的示例中，允许共享slot，可以将示例作业的并发度从2增加到6，从而可以充分利用资源，同时保证负载很重的subtasks可以在TaskManagers中平均分配。</p>

<p><img src="./../assets/img/slot_sharing.svg" alt="" /></p>

<p>API s还包含了一种 <em>资源组（resource group）</em> 机制，用来防止不必要的slot sharing。</p>

<p>经验来讲，task slots的默认值应该与CPU核数一致。在使用超线程下，一个slot将会占用2个或更多的硬件资源。</p>

<h3 id="44-状态管理-state-backends">4.4. <a name="StateBackends"></a>状态管理 State Backends</h3>

<p>key/values 索引存储的准确数据结构取决于选择的 state backend。其中一个 state backend将数据存储在内存hash map中，另一个 state backend使用RocksDB作为key/value 存储。 除了定义存储状态的数据结构， state backends还实现了获取 key/value状态的时间点快照的逻辑，并将该快照存储为checkpoint的一部分。</p>

<p><img src="./../assets/img/checkpoints.svg" alt="" /></p>

<h3 id="45-savepoints">4.5. <a name="Savepoints"></a>Savepoints</h3>

<p>使用DataStream API 编写的程序可以从一个savepoint恢复执行。Savepoints允许在不丢失任何状态的情况下修改程序和Flink集群。</p>

<p>Savepoints 是手动触发的checkpoints，它依赖常规的checkpointing机制，生成程序快照并将其写入到状态后端。在运行期间，worker节点周期性的生成程序快照并产生checkpoints。在恢复重启时只会使用最后成功的checkpoint。并且只要有一个新的checkpoint生成时，旧的checkpoints将会被安全地丢弃。</p>

<p>Savepoints与周期性触发的checkpoints很类似，但是其式由由用户触发的，且当更新的checkpoints完成时，老的checkpoint不会自动失效。可以通过命令行或者在取消一个job时调用REST API 的方式创建Savepoints。</p>

<h2 id="5-checkpoint">5. <a name="checkpoint-1"></a>checkpoint</h2>

<blockquote>
  <p>checkpoint使Fink的状态具有非常好的容错性，通过Checkpoint，Flink可以对作业的状态和计算位置进行恢复，因此Flink作业具备高容错执行语意。 通过 Checkpointing 查看如何在程序中开启和配置checkpoint。</p>
</blockquote>

<p><code>保留Checkpoint</code></p>

<p>默认情况下，Checkpoint仅用于恢复失败的作业，是不保留的，程序结束时Checkpoints也会被删除。然而，你可以配置周期性的保留checkpoint。当作业失败或被取消时，这些checkpoints将不会被自动清除。这样，你就可以用该checkpoint来恢复失败的作业。</p>

<pre><code class="language-java">
CheckpointConfig config = env.getCheckpointConfig();
config.enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);
</code></pre>

<p>“ExternalizedCheckpointCleanup”配置项定义了当作业取消时，对作业checkpoints的操作：</p>

<ul>
  <li>ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION: 作业取消时，保留作业的checkpoint。注意，这种情况下，需要手动清除该作业的checkpoint。</li>
  <li>ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION: 作业取消时，删除作业的checkpoint。仅当作业失败时，作业的checkpoint才会被使用。</li>
</ul>

<p><code>目录结构</code></p>

<p>与 savepoints 类似， checkpoint由元数据文件、额外的数据文件（与state backend相关）组成。可以通过配置文件中“state.checkpoints.dir”配置项，指定元数据文件和数据文件的存储路径，也可以在代码中针对单个作业指定该配置。</p>

<ul>
  <li>通过配置文件全局配置</li>
</ul>

<pre><code class="language-shell">state.checkpoints.dir: hdfs:///checkpoints/
</code></pre>
<p>创建state backend时对单个作业进行配置#</p>

<pre><code class="language-java">env.setStateBackend(new RocksDBStateBackend("hdfs:///checkpoints-data/");
</code></pre>

<p><code>checkpoint与savepoint的区别</code></p>

<p>checkpoint与savepoint有一些区别。 他们</p>

<p>-的数据格式与state backend密切相关，可能以增量方式存储。</p>
<ul>
  <li>不支持Flink的特殊功能，如扩缩容。
<code>从checkpoint中恢复状态</code></li>
</ul>

<p>同savepoint一样，作业也可以使用checkpoint的元数据文件进行错误恢复。注意若元数据文件中信息不够，那么jobmanager就需要使用相关的数据文件来恢复作业。</p>
<pre><code class="language-shell">$ bin/flink run -s :checkpointMetaDataPath [:runArgs]
</code></pre>

<hr />

<h2 id="6-savepoint">6. <a name="Savepoint"></a>Savepoint</h2>

<blockquote>
  <p>Savepoints是存储在外部文件系统的的自完备的checkpoints，可以用来停止-恢复或升级Flink程序。其使用Flink的 checkpoint机制 创建流作业的全量（非增量）状态快照，并且将checkpoint数据和元数据写出到外部文件系统。</p>
</blockquote>

<p><code>分配算子ID</code></p>

<p><strong>墙裂推荐</strong>   通过“uid(String)”手动指定算子ID。这些ID将在确定每个算子的状态时使用。</p>

<pre><code class="language-java"> DataStream &lt;String&gt; stream = env.
  // Stateful source (e.g. Kafka) with ID
  .addSource(new StatefulSource())
  .uid("source-id") // ID for the source operator
  .shuffle()
  // Stateful mapper with ID
  .map(new StatefulMapper())
  .uid("mapper-id") // ID for the mapper
  // Stateless printing sink
  .print(); // Auto-generated ID
</code></pre>

<p>如果不手动指定算子ID，ID将自动生成。只要这些ID不改变，就可以从savepoint中自动恢复状态。自动生成的ID依赖于程序的结构，并且非常容易受到程序变化的影响。因此，墙裂推荐手动指定ID。</p>

<p><code>Savepoint状态</code></p>

<p>你可以将savepoint想象成为保存了每个有状态的算子的“算子ID-&gt;状态”映射的集合。</p>
<pre><code>Operator ID | State
------------+------------------------
source-id   | State of StatefulSource
mapper-id   | State of StatefulMapper
</code></pre>
<p>在上述示例中，print结果表是无状态的，因此不是savepoint状态的一部分。默认情况下，我们试图将savepoint的每条数据，都映射到新的程序中。</p>

<h1 id="stream-processing-fundamentals">stream processing fundamentals</h1>

<h1 id="architecture-of-apache-flink">architecture of Apache Flink</h1>

<blockquote>
  <p>为什么 Flink 能同时支持批处理和流处理呢？
Flink将批处理（即静态和有限数据集合的处理）视为流处理的一种特例。</p>
</blockquote>

<p>Flink 是一个分层设计的系统。栈中的不同层次构建在彼此之上，并提供不同的抽象级别的功能。</p>

<p>从下至上：</p>

<ul>
  <li>Runtime 层以 JobGraph 形式接收程序。JobGraph 即为一个一般化的并行有向无环数据流图（data flow），它拥有任意数量的 Task 来接收和产生 DataStream。</li>
  <li>DataStream API 和 DataSet API 都会使用单独编译的方式生成JobGraph。 DataSet API 使用 optimizer 来决定针对程序的优化方法，而 DataStream API 则使用 stream builder 来完成该任务。</li>
  <li>在执行JobGraph时，Flink提供了多种候选部署方案（如local，remote，YARN，云端部署等）。</li>
  <li>Flink 附随了一些产生 DataSet 或 DataStream API 程序的的类库和 API ：处理逻辑表查询的Table，机器学习的FlinkML，图像处理的Gelly，复杂事件处理的CEP等。</li>
</ul>

<p><img src="./../assets/img/component-stack.png" alt="" /></p>

<p><code>Runtime</code></p>

<p>标识为“Runtime”的是Flink的分布式运行环境，是Flink核心计算结构系统，它接受流数据流程序并在一台或多台机器中以容错方式执行它们。</p>

<ul>
  <li>Rumtime 接受的程序非常强大，但是直接基于它编程很困难。出于这个原因，Flink提供了开发人员友好的 API ，这些 API 在 Rumtime 层之上进行分层并生成这些流数据流程序。有用于流处理的 DataStream API 和用于批处理的 DataSet API 。值得注意的是，虽然Flink的运行时总是基于流，但 DataSet API 早于 DataStream API ，因为处理无限流的行业需求在第一个Flink年代并没有那么广泛。</li>
</ul>

<p><img src="./../assets/img/core-job-design.png" alt="" /></p>

<h1 id="架构演化">架构演化</h1>

<h2 id="7-time">7. <a name="time-1"></a><code>time</code></h2>

<p>Flink 在流程序中支持不同的 Time 概念</p>
<ul>
  <li>Processing Time</li>
  <li>Event Time</li>
  <li>Ingestion Time</li>
</ul>

<p>下面我们一起来看看这几个 Time：</p>

<p><code>Processing Time</code></p>

<blockquote>
  <p>Processing Time 是指事件被处理时机器的系统时间。</p>
</blockquote>

<p>当流程序在 Processing Time 上运行时，所有基于时间的操作(如时间窗口)将使用当时机器的系统时间。每小时 Processing Time 窗口将包括在系统时钟指示整个小时之间到达特定操作的所有事件。</p>

<p>例如，如果应用程序在上午 9:15 开始运行，则第一个每小时 Processing Time 窗口将包括在上午 9:15 到上午 10:00 之间处理的事件，下一个窗口将包括在上午 10:00 到 11:00 之间处理的事件。</p>

<p>Processing Time 是最简单的 “Time” 概念，不需要流和机器之间的协调，它提供了最好的性能和最低的延迟。但是，在分布式和异步的环境下，Processing Time 不能提供确定性，因为它容易受到事件到达系统的速度（例如从消息队列）、事件在系统内操作流动的速度以及中断的影响。</p>

<p><code>Event Time</code></p>

<blockquote>
  <p>Event Time 是事件发生的时间，一般就是数据本身携带的时间。这个时间通常是在事件到达 Flink 之前就确定的，并且可以从每个事件中获取到事件时间戳。在 Event Time 中，时间取决于数据，而跟其他没什么关系。Event Time 程序必须指定如何生成 Event Time 水印(Watermark)，这是表示 Event Time 进度的机制。</p>
</blockquote>

<p>在按业务发生时间统计数据时，面临一个问题，当接收数据的时间是无序的时候，我们什么时间去触发聚合计算，不可能无限制的等待。Flink引入了Watermark的概念，它是给窗口看的，是告诉窗口最长等待的时间是多久，超过这个时间的数据就抛弃不再处理。</p>

<p>完美的说，无论事件什么时候到达或者其怎么排序，最后处理 Event Time 将产生完全一致和确定的结果。但是，除非事件按照已知顺序（按照事件的时间）到达，否则处理 Event Time 时将会因为要等待一些无序事件而产生一些延迟。由于只能等待一段有限的时间，因此就难以保证处理 Event Time 将产生完全一致和确定的结果。</p>

<p>假设所有数据都已到达， Event Time 操作将按照预期运行，即使在处理无序事件、延迟事件、重新处理历史数据时也会产生正确且一致的结果。 例如，每小时事件时间窗口将包含带有落入该小时的事件时间戳的所有记录，无论它们到达的顺序如何。</p>

<p>请注意，有时当 Event Time 程序实时处理实时数据时，它们将使用一些 Processing Time 操作，以确保它们及时进行。</p>

<p><code>Ingestion Time</code></p>

<blockquote>
  <p>Ingestion Time 是事件进入 Flink 的时间。 在源操作处，每个事件将源的当前时间作为时间戳，并且基于时间的操作（如时间窗口）会利用这个时间戳。</p>
</blockquote>

<p>Ingestion Time 在概念上位于 Event Time 和 Processing Time 之间。 与 Processing Time 相比，它稍微贵一些，但结果更可预测。因为 Ingestion Time 使用稳定的时间戳（在源处分配一次），所以对事件的不同窗口操作将引用相同的时间戳，而在 Processing Time 中，每个窗口操作符可以将事件分配给不同的窗口（基于机器系统时间和到达延迟）。</p>

<p>与 Event Time 相比，Ingestion Time 程序无法处理任何无序事件或延迟数据，但程序不必指定如何生成水印。</p>

<p>在 Flink 中，Ingestion Time 与 Event Time 非常相似，但 Ingestion Time 具有自动分配时间戳和自动生成水印功能。</p>

<p><code>Event Time 和 Watermarks</code></p>

<blockquote>
  <p>支持 Event Time 的流处理器需要一种方法来衡量 Event Time 的进度。 例如，当 Event Time 超过一小时结束时，需要通知构建每小时窗口的窗口操作符，以便操作员可以关闭正在进行的窗口。</p>
</blockquote>

<p>Event Time 可以独立于 Processing Time 进行。 例如，在一个程序中，操作员的当前 Event Time 可能略微落后于 Processing Time （考虑到接收事件的延迟），而两者都以相同的速度进行。另一方面，另一个流程序可能只需要几秒钟的时间就可以处理完 Kafka Topic 中数周的 Event Time 数据。</p>

<p>Flink 中用于衡量 Event Time 进度的机制是 Watermarks。 Watermarks 作为数据流的一部分流动并带有时间戳 t。 Watermark（t）声明 Event Time 已到达该流中的时间 t，这意味着流中不应再有具有时间戳 t’&lt;= t 的元素（即时间戳大于或等于水印的事件）</p>

<p>下图显示了带有(逻辑)时间戳和内联水印的事件流。在本例中，事件是按顺序排列的(相对于它们的时间戳)，这意味着水印只是流中的周期性标记。</p>

<h2 id="8-window">8. <a name="window-1"></a><code>window</code></h2>

<blockquote>
  <p>Window 就是用来对一个无限的流设置一个有限的集合，在有界的数据集上进行操作的一种机制。</p>
</blockquote>

<p>Window 又可以分为</p>

<ul>
  <li>基于时间（Time-based）的 window</li>
  <li>基于数量（Count-based）的 window</li>
</ul>

<p><code>time-based </code> window</p>

<blockquote>
  <p>Time Windows 根据时间来聚合流数据。例如：一分钟的 tumbling time window 收集一分钟的元素，并在一分钟过后对窗口中的所有元素应用于一个函数。</p>
</blockquote>

<ul>
  <li>tumbling time windows(翻滚时间窗口)</li>
  <li>sliding time windows(滑动时间窗口)</li>
</ul>

<p>tumbling time windows</p>

<pre><code class="language-java"> DataStream .keyBy(1)
    .timeWindow(Time.minutes(1)) //tumbling time window 每分钟统计一次数量和
    .sum(1);
</code></pre>

<p>sliding time windows
输入两个时间参数</p>

<pre><code class="language-java"> DataStream .keyBy(1)
	.timeWindow(Time.minutes(1), Time.seconds(30)) //sliding time window 每隔 30s 统计过去1分钟的数量和
	.sum(1);
</code></pre>

<p><code>Count-based</code> window</p>

<blockquote>
  <p>Apache Flink 还提供计数窗口功能。如果计数窗口设置的为 100 ，那么将会在窗口中收集 100 个事件，并在添加第 100 个元素时计算窗口的值。</p>
</blockquote>

<ul>
  <li>tumbling count window</li>
  <li>sliding count window</li>
</ul>

<p>tumbling count window
输入一个时间参数</p>

<pre><code class="language-java">data.keyBy(1)
	.countWindow(100) //统计每 100 个元素的数量之和
	.sum(1);
</code></pre>

<p>sliding count window</p>

<p>输入两个时间参数</p>

<pre><code class="language-java">data.keyBy(1)
	.countWindow(100, 10) //每 10 个元素统计过去 100 个元素的数量之和
	.sum(1);
</code></pre>
<h1 id="开发环境准备相关">开发环境准备相关</h1>

<p>一个好的 IDE 不仅能有效的提高开发者的开发效率，而且对于不做代码开发但是希望通过代码学 习 Flink 的人来说，也非常有助于其对代码的理解。
推荐使用 IntelliJ IDEA IDE 作为 Flink 的 IDE 工具。
官方的说法是，不建议使用 Eclipse IDE，主 要原因是 Eclipse 的 Scala IDE 和 Flink 用 scala 的不兼容。</p>

<h3 id="安装-scala-plugin">安装 Scala plugin</h3>
<p>Flink 项目使用了 Java 和 Scala 开发，Intellij 自带 Java 的支持，在导入 Flink 代码前，还需要确 保安装 Intellij 的 Scala plugin。安装方法如下:</p>
<ol>
  <li>IntelliJ IDEA -&gt; Preferences -&gt; Plugins，点击“Install Jetbrains plugin…”</li>
  <li>搜索“scala”，点击“install”</li>
  <li>重启 Intellij</li>
</ol>

<h3 id="添加-java-的-checkstyle">添加 Java 的 Checkstyle</h3>
<p>在 Intellij 中添加 Checkstyle 是很重要的，因为 Flink 在编译时会强制代码风格的检查，如果代码 风格不符合规范，可能会直接编译失败。对于需要在开源代码基础上做二次开发的同学，或者有 志于向社区贡献代码的同学来说，及早添加 checkstyle 并注意代码规范，能帮你节省不必要的修 改代码格式的时间。
Intellij 内置对 Checkstyle 的支持，可以检查一下 Checkstyle-IDEA plugin 是否安装(IntelliJ IDEA -&gt; Preferences -&gt; Plugins，搜索“Checkstyle-IDEA”)。
配置 Java Checkstyle:</p>
<ol>
  <li>IntelliJ IDEA -&gt; Preferences -&gt; Other Settings -&gt; Checkstyle</li>
  <li>设置 “Scan Scope”为“Only Java sources (including tests)”</li>
  <li>在“Checkstyle Version”下拉框中选择“8.9”</li>
  <li>
    <p>在“Configuration File”中点击“+”新增一个 flink 的配置:
a. “Description”填“Flink”
b. “Use a local Checkstyle file”选择本代码下的 tools/maven/checkstyle.xml 文件
c. 勾选“Store relative to project location”，然后点击“Next”
d. 配置“checkstyle.suppressions.file” 的值为”suppressions.xml”，然后点击“Next”和
“Finish”
e. 勾选上“Flink”作为唯一生效的 checkstyle 配置，点击“Apply”和“OK”</p>
  </li>
  <li>IntelliJ IDEA -&gt; Preferences -&gt; Editor -&gt; Code Style -&gt; Java，点击
⚙齿轮按钮，选择 “Import Scheme” -&gt; “Checkstyle Configuration”，选择 checkstyle.xml 文件。这样配置后，
 Intellij 在自动 import 的时候会按照规则，把 import 代码添加到正确的位置。
需要说明的是，Flink 中的一些模块并不能完全 checkstyle 通过，包括 flink-core、flink-optimizer 和 flink-runtime。但无论如何，还是应当保证你新增或修改的代码遵守 checkstyle 的规范。
    <h3 id="添加-scala-的-checkstyle">添加 Scala 的 Checkstyle</h3>
  </li>
  <li>将“tools/maven/scalastyle-config.xml”文件拷贝到 flink 代码根目录的“.idea”子目录中</li>
  <li>IntelliJ IDEA -&gt; Preferences -&gt; Editor -&gt; Inspections，搜索“Scala style inspections”，勾选这一项</li>
</ol>

	  ]]></description>
	</item>

	<item>
	  <title>I Have a To Do</title>
	  <link>/2018-03-05-to-do/</link>
	  <author></author>
	  <pubDate>2018-03-05T18:18:00+08:00</pubDate>
	  <guid>/2018-03-05-to-do/</guid>
	  <description><![CDATA[
	     <h1 id="目录">目录</h1>

<ul>
  <li><a href="#param-and-attribute">param and attribute</a></li>
  <li><a href="#to-do-list">Get the things done</a></li>
</ul>

<hr />

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby">def foo
  puts &#39;foo&#39;
end</code></pre></figure>

<hr />

<p>入职手续准备
遗留问题研究</p>

<p>敬慎</p>
<ul>
  <li>高而能下，</li>
  <li>满而能虚，</li>
  <li>富而能俭，</li>
  <li>贵而能卑，</li>
  <li>智而能愚，</li>
  <li>勇而能怯，</li>
  <li>辩而能讷，</li>
  <li>博而能浅，</li>
  <li>明而能闇，</li>
  <li>是谓损而不极。</li>
  <li>能行此道，唯至德者及之。</li>
</ul>

<h3 id="各种集合的-遍历性能对比">各种集合的 遍历性能对比</h3>

<p>可以在 github 上 开一个
先想想怎么测试</p>

<ul>
  <li>数据量
    <ul>
      <li>基本类型</li>
      <li>对象</li>
      <li>大对象</li>
    </ul>
  </li>
  <li>测试对象
    <ul>
      <li>list
        <ul>
          <li>arrayList</li>
          <li>linkedList</li>
          <li></li>
        </ul>
      </li>
      <li>map</li>
      <li>set</li>
      <li>vector</li>
    </ul>
  </li>
</ul>

<h3 id="to-do-listdo-to-list">to do list(do to list)</h3>
<ul>
  <li>spring 画面验证
    <ul>
      <li>Valid</li>
      <li>Validated 区别验证</li>
    </ul>
  </li>
  <li>
    <p>Annotation 会覆盖嘛？</p>
  </li>
  <li>db
    <ul>
      <li>redis</li>
      <li>mybatis generator</li>
      <li>mybatis 二级缓存</li>
    </ul>
  </li>
</ul>

<h3 id="异常处理最佳实践">异常处理最佳实践</h3>
<p><code>根据我的工作经历来看，我主要遵循以下几点：</code>
    1. 尽量不要在代码中写try…catch.finally把异常吃掉。
    2. 异常要尽量直观，防止被他人误解
    3. 将异常分为以下几类,业务异常，登录状态无效异常，（虽已登录，且状态有效）未授权异常，系统异常（JDK中定义Error和Exception，比如NullPointerException, ArithmeticException 和 InputMismatchException）
    4. 可以在某个特定的Controller中处理异常，也可以使用全局异常处理器。尽量使用全局异常处理器</p>

<ul>
  <li>spring
    <ul>
      <li>controller</li>
      <li>validator</li>
      <li>command</li>
      <li>form</li>
      <li>model</li>
      <li>dispatcherServlet</li>
      <li>handler mapping</li>
      <li>view resolver</li>
    </ul>
  </li>
</ul>

<hr />

<p>WebApplicationContext</p>

<table>
  <thead>
    <tr>
      <th>No</th>
      <th>Bean Type</th>
      <th>Explanation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>HandlerMapping</td>
      <td>Maps incoming requests to handlers and a list of pre- and post-processors (handler interceptors) based on some criteria the details of which vary by HandlerMapping implementation. The most popular implementation supports annotated controllers but other implementations exists as well.</td>
    </tr>
    <tr>
      <td>2</td>
      <td>HandlerAdapter</td>
      <td>Helps the DispatcherServlet to invoke a handler mapped to a request regardless of the handler is actually invoked. For example, invoking an annotated controller requires resolving various annotations. Thus the main purpose of a HandlerAdapter is to shield the DispatcherServlet from such details.</td>
    </tr>
    <tr>
      <td>3</td>
      <td>HandlerExceptionResolver</td>
      <td>Maps exceptions to views also allowing for more complex exception handling code.</td>
    </tr>
    <tr>
      <td>4</td>
      <td>ViewResolver</td>
      <td>Resolves logical String-based view names to actual View types.</td>
    </tr>
    <tr>
      <td>5</td>
      <td>LocaleResolver &amp; LocaleContextResolver</td>
      <td>Resolves the locale a client is using and possibly their time zone, in order to be able to offer internationalized views</td>
    </tr>
    <tr>
      <td>6</td>
      <td>ThemeResolver</td>
      <td>Resolves themes your web application can use, for example, to offer personalized layouts</td>
    </tr>
    <tr>
      <td>7</td>
      <td>MultipartResolver</td>
      <td>Parses multi-part requests for example to support processing file uploads from HTML forms.</td>
    </tr>
    <tr>
      <td>8</td>
      <td>FlashMapManager</td>
      <td>Stores and retrieves the “input” and the “output” FlashMap that can be used to pass attributes from one request to another, usually across a redirect.</td>
    </tr>
  </tbody>
</table>

<hr />

<h3 id="request-中-parameter-与-attribute-的区别param-and-attribute">request 中 parameter 与 attribute 的区别(param and attribute)</h3>
<ul>
  <li>不同点
    <ul>
      <li>来源：param 为 客户端发送给服务端的参数 而 attribute 是服务端设置的 属性</li>
      <li>操作：param 只能读取，不能设置，而 attribute 能够被读写</li>
      <li>类型：param 作为参数，放在 get 方式的 url 中 post 方式的 request body 中，在服务端获取的时候，都会被当做 string 看待, 而 attribute 则可以存储任意的 object 对象</li>
    </ul>
  </li>
  <li>共同点
    <ul>
      <li>二者的值都被封装在request对象中。</li>
    </ul>
  </li>
</ul>

<h3 id="异常处理最佳实践-1">异常处理最佳实践</h3>

<h3 id="登陆">登陆</h3>

<h3 id="行程计划">行程计划</h3>

<ul>
  <li>插入</li>
  <li>更新</li>
  <li>查询</li>
  <li>分页</li>
  <li>流水记录</li>
  <li>流水查询</li>
  <li>流水能更新</li>
  <li>流水设计</li>
  <li>流水插入时点</li>
  <li>行程计划和流水查询的相关功能点， 哪里需要拉计划和流水，找出来</li>
</ul>

<h4 id="ide运行">IDE运行</h4>

<p>因为程序本身按照Docker启动，所以对于hostname需要在hosts文件中设置正确才能正常运行：</p>

<pre><code class="language-shell">## solar
127.0.0.1 eureka1
127.0.0.1 eureka2
127.0.0.1 rabbitmq
127.0.0.1 zipkin_server
127.0.0.1 solar_mysql
127.0.0.1 gitlab
</code></pre>

<p>根据依赖关系，程序最好按照以下的顺序执行</p>

<p>docker mysql &gt; docker rabbitmq &gt; eureka server &gt; config server &gt; zipkin server &gt; 其他业务微服务（account-ms, product-ms, order-ms, tcc-coordinator-ms等）</p>

<h2 id="示例">示例</h2>

<p>根据附表中的服务字典，我们通过Zuul或Swagge对<code>order</code>服务进行预订单生成操作。</p>

<pre><code class="language-http">POST http://localhost:7291/order/api/v1/orders
Content-Type: application/json;charset=UTF-8

{
  "product_id": 7,
  "user_id": 1
}
</code></pre>

<p>成功后我们将得到预订单的结果</p>

<pre><code class="language-json">{
  "data": {
    "id": 15,
    "create_time": "2017-03-28T18:18:02.206+08:00",
    "update_time": "1970-01-01T00:00:00+08:00",
    "delete_time": "1970-01-01T00:00:00+08:00",
    "user_id": 1,
    "product_id": 7,
    "price": 14,
    "status": "PROCESSING"
  },
  "code": 20000
}
</code></pre>

<p>此时我们再确认订单</p>

<p>(如果想测试预留资源的补偿情况，那么就等15s后过期再发请求，注意容器与宿主机的时间)</p>

<pre><code>POST http://localhost:7291/order/api/v1/orders/confirmation
Content-Type: application/json;charset=UTF-8

{
  "order_id": 15
}
</code></pre>

<p>如果成功确认则返回如下结果</p>

<pre><code class="language-json">{
  "data": {
    "id": 15,
    "create_time": "2017-03-28T18:18:02.206+08:00",
    "update_time": "2017-03-28T18:21:32.78+08:00",
    "delete_time": "1970-01-01T00:00:00+08:00",
    "user_id": 1,
    "product_id": 7,
    "price": 14,
    "status": "DONE"
  },
  "code": 20000
}
</code></pre>

<pre><code class="language-java">public class RPCProtocol implements Protocol {
	
	public static final int TYPE = 1;
	
	private static final Log LOGGER = LogFactory.getLog(RPCProtocol.class);
	
	private static final int REQUEST_HEADER_LEN = 1 * 6 + 5 * 4;
	
	private static final int RESPONSE_HEADER_LEN = 1 * 6 + 3 * 4;
	
	private static final byte VERSION = (byte)1;
	
	private static final byte REQUEST = (byte)0;
	
	private static final byte RESPONSE = (byte)1;
	
	@Override
	public ByteBufferWrapper encode(Object message, ByteBufferWrapper bytebufferWrapper) throws Exception{
		if(!(message instanceof RequestWrapper) &amp;&amp; !(message instanceof ResponseWrapper)){
			throw new Exception("only support send RequestWrapper &amp;&amp; ResponseWrapper");
		}
		int id = 0;
		byte type = REQUEST;
		if(message instanceof RequestWrapper){
			try{
				int requestArgTypesLen = 0;
				int requestArgsLen = 0;
				List&lt;byte[]&gt; requestArgTypes = new ArrayList&lt;byte[]&gt;();
				List&lt;byte[]&gt; requestArgs = new ArrayList&lt;byte[]&gt;();
				RequestWrapper wrapper = (RequestWrapper) message;
				byte[][] requestArgTypeStrings = wrapper.getArgTypes();
				for (byte[] requestArgType : requestArgTypeStrings) {
					requestArgTypes.add(requestArgType);
					requestArgTypesLen += requestArgType.length;
				}
				Object[] requestObjects = wrapper.getRequestObjects();
				if(requestObjects!=null){
					for (Object requestArg : requestObjects) {
						byte[] requestArgByte = Codecs.getEncoder(wrapper.getCodecType()).encode(requestArg);
						requestArgs.add(requestArgByte);
						requestArgsLen += requestArgByte.length;
					}
				}
				byte[] targetInstanceNameByte = wrapper.getTargetInstanceName();
				byte[] methodNameByte = wrapper.getMethodName();
				id = wrapper.getId();
				int timeout = wrapper.getTimeout();
				int capacity = ProtocolUtils.HEADER_LEN + REQUEST_HEADER_LEN + requestArgs.size() * 4 * 2 + targetInstanceNameByte.length 
							   + methodNameByte.length + requestArgTypesLen + requestArgsLen;
				ByteBufferWrapper byteBuffer = bytebufferWrapper.get(capacity);
				byteBuffer.writeByte(ProtocolUtils.CURRENT_VERSION);
				byteBuffer.writeByte((byte)TYPE);
				byteBuffer.writeByte(VERSION);
				byteBuffer.writeByte(type);
				byteBuffer.writeByte((byte)wrapper.getCodecType());
				byteBuffer.writeByte((byte)0);
				byteBuffer.writeByte((byte)0);
				byteBuffer.writeByte((byte)0);
				byteBuffer.writeInt(id);
				byteBuffer.writeInt(timeout);
				byteBuffer.writeInt(targetInstanceNameByte.length);
				byteBuffer.writeInt(methodNameByte.length);
				byteBuffer.writeInt(requestArgs.size());
				for (byte[] requestArgType : requestArgTypes) {
					byteBuffer.writeInt(requestArgType.length);
				}
				for (byte[] requestArg : requestArgs) {
					byteBuffer.writeInt(requestArg.length);
				}
				byteBuffer.writeBytes(targetInstanceNameByte);
				byteBuffer.writeBytes(methodNameByte);
				for (byte[] requestArgType : requestArgTypes) {
					byteBuffer.writeBytes(requestArgType);
				}
				for (byte[] requestArg : requestArgs) {
					byteBuffer.writeBytes(requestArg);
				}
				return byteBuffer;
			}
			catch(Exception e){
				LOGGER.error("encode request object error",e);
				throw e;
			}
		}
		else{
			ResponseWrapper wrapper = (ResponseWrapper) message;
			byte[] body = new byte[0];
			byte[] className = new byte[0];
			try{
				// no return object
				if(wrapper.getResponse() != null){
					className = wrapper.getResponse().getClass().getName().getBytes();
					body = Codecs.getEncoder(wrapper.getCodecType()).encode(wrapper.getResponse());
				}
				if(wrapper.isError()){
					className = wrapper.getException().getClass().getName().getBytes();
					body = Codecs.getEncoder(wrapper.getCodecType()).encode(wrapper.getException());
				}
				id = wrapper.getRequestId();
			}
			catch(Exception e){
				LOGGER.error("encode response object error", e);
				// still create responsewrapper,so client can get exception
				wrapper.setResponse(new Exception("serialize response object error",e));
				className = Exception.class.getName().getBytes();
				body = Codecs.getEncoder(wrapper.getCodecType()).encode(wrapper.getResponse());
			}
			type = RESPONSE;
			int capacity = ProtocolUtils.HEADER_LEN + RESPONSE_HEADER_LEN + body.length;
			if(wrapper.getCodecType() == Codecs.PB_CODEC){
				capacity += className.length;
			}
			ByteBufferWrapper byteBuffer = bytebufferWrapper.get(capacity);
			byteBuffer.writeByte(ProtocolUtils.CURRENT_VERSION);
			byteBuffer.writeByte((byte)TYPE);
			byteBuffer.writeByte(VERSION);
			byteBuffer.writeByte(type);
			byteBuffer.writeByte((byte)wrapper.getCodecType());
			byteBuffer.writeByte((byte)0);
			byteBuffer.writeByte((byte)0);
			byteBuffer.writeByte((byte)0);
			byteBuffer.writeInt(id);
			if(wrapper.getCodecType() == Codecs.PB_CODEC){
				byteBuffer.writeInt(className.length);
			}
			else{
				byteBuffer.writeInt(0);
			}
			byteBuffer.writeInt(body.length);
			if(wrapper.getCodecType() == Codecs.PB_CODEC){
				byteBuffer.writeBytes(className);
			}
			byteBuffer.writeBytes(body);
			return byteBuffer;
		}
	}
	
	public Object decode(ByteBufferWrapper wrapper,Object errorObject,int...originPosArray) throws Exception{
		final int originPos;
		if(originPosArray!=null &amp;&amp; originPosArray.length == 1){
			originPos = originPosArray[0];
		}
		else{
			originPos = wrapper.readerIndex();
		}
		if(wrapper.readableBytes() &lt; 2){
			wrapper.setReaderIndex(originPos);
        	return errorObject;
        }
        byte version = wrapper.readByte();
        if(version == (byte)1){
        	byte type = wrapper.readByte();
        	if(type == REQUEST){
        		if(wrapper.readableBytes() &lt; REQUEST_HEADER_LEN -2){
        			wrapper.setReaderIndex(originPos);
        			return errorObject;
        		}
        		int codecType = wrapper.readByte();
        		wrapper.readByte();
        		wrapper.readByte();
        		wrapper.readByte();
        		int requestId = wrapper.readInt();
        		int timeout = wrapper.readInt();
        		int targetInstanceLen = wrapper.readInt();
        		int methodNameLen = wrapper.readInt();
        		int argsCount = wrapper.readInt();
        		int argInfosLen = argsCount * 4 * 2;
        		int expectedLenInfoLen = argInfosLen + targetInstanceLen + methodNameLen;
        		if(wrapper.readableBytes() &lt; expectedLenInfoLen){
        			wrapper.setReaderIndex(originPos);
        			return errorObject;
        		}
        		int expectedLen = 0;
        		int[] argsTypeLen = new int[argsCount];
        		for (int i = 0; i &lt; argsCount; i++) {
					argsTypeLen[i] = wrapper.readInt();
					expectedLen += argsTypeLen[i]; 
				}
        		int[] argsLen = new int[argsCount];
        		for (int i = 0; i &lt; argsCount; i++) {
        			argsLen[i] = wrapper.readInt();
        			expectedLen += argsLen[i];
				}
        		byte[] targetInstanceByte = new byte[targetInstanceLen];
        		wrapper.readBytes(targetInstanceByte);
        		byte[] methodNameByte = new byte[methodNameLen];
        		wrapper.readBytes(methodNameByte);
        		if(wrapper.readableBytes() &lt; expectedLen){
        			wrapper.setReaderIndex(originPos);
        			return errorObject;
        		}
        		byte[][] argTypes = new byte[argsCount][];
        		for (int i = 0; i &lt; argsCount; i++) {
					byte[] argTypeByte = new byte[argsTypeLen[i]];
					wrapper.readBytes(argTypeByte);
					argTypes[i] = argTypeByte;
				}
        		Object[] args = new Object[argsCount];
        		for (int i = 0; i &lt; argsCount; i++) {
					byte[] argByte = new byte[argsLen[i]];
					wrapper.readBytes(argByte);
					args[i] = argByte;
				}
        		RequestWrapper requestWrapper = new RequestWrapper(targetInstanceByte, methodNameByte, 
        														   argTypes, args, timeout, requestId, codecType, TYPE);
        		int messageLen = ProtocolUtils.HEADER_LEN + REQUEST_HEADER_LEN + expectedLenInfoLen + expectedLen;
        		requestWrapper.setMessageLen(messageLen);
        		return requestWrapper;
        	}
        	else if(type == RESPONSE){
        		if(wrapper.readableBytes() &lt; RESPONSE_HEADER_LEN -2){
        			wrapper.setReaderIndex(originPos);
        			return errorObject;
        		}
        		int codecType = wrapper.readByte();
        		wrapper.readByte();
        		wrapper.readByte();
        		wrapper.readByte();
            	int requestId = wrapper.readInt();
            	int classNameLen = wrapper.readInt();
            	int bodyLen = wrapper.readInt();
            	if(wrapper.readableBytes() &lt; classNameLen + bodyLen){
            		wrapper.setReaderIndex(originPos);
            		return errorObject;
            	}

            	byte[] classNameBytes = null;
            	if(codecType == Codecs.PB_CODEC){	
	            	classNameBytes = new byte[classNameLen];
	            	wrapper.readBytes(classNameBytes);
            	}
            	byte[] bodyBytes = new byte[bodyLen];
            	wrapper.readBytes(bodyBytes);
            	ResponseWrapper responseWrapper = new ResponseWrapper(requestId,codecType,TYPE);
            	responseWrapper.setResponse(bodyBytes);
            	responseWrapper.setResponseClassName(classNameBytes);
	        	int messageLen = ProtocolUtils.HEADER_LEN + RESPONSE_HEADER_LEN + classNameLen + bodyLen;
	        	responseWrapper.setMessageLen(messageLen);
            	return responseWrapper;
        	}
        	else{
        		throw new UnsupportedOperationException("protocol type : "+type+" is not supported!");
        	}
        }
        else{
        	throw new UnsupportedOperationException("protocol version :"+version+" is not supported!");
        }
	}

}
</code></pre>

<p>至此就完成了一次TCC事务，当然你也可以测试超时和冲突的情况，这里就不再赘述。</p>

<h2 id="拓展">拓展</h2>

<h3 id="使用gitlab作为远程配置仓库">使用Gitlab作为远程配置仓库</h3>

<p>本例中默认使用Github或GitOsc中的公开仓库，出于自定义的需要，我们可以在本地构建Git仓库，这里选用Gitlab为例。</p>

<p>将以下配置添加至docker compose中的文件中并启动Docker Gitlab容器：</p>

<pre><code class="language-yaml">gitlab:
    image: daocloud.io/daocloud/gitlab:8.16.7-ce.0
    ports:
        - "10222:22"
        - "80:80"
        - "10443:443"
    volumes:
        - "./docker-gitlab/config/:/etc/gitlab/"
        - "./docker-gitlab/logs/:/var/log/gitlab/"
        - "./docker-gitlab/data/:/var/opt/gitlab/"
    environment:
        - TZ=Asia/Shanghai
</code></pre>

<p>将项目的<code>config-repo</code>添加至Gitlab中，并修改<code>config-ms</code>中git仓库的相关验证等参数即可。</p>

<p><img src="./../assets/images/cover3.jpg" alt="" /></p>

<h2 id="服务字典">服务字典</h2>

<p>鉴于Spring Boot Actuator的端点所带来的两面性，除了可以增加<code>spring-boot-starter-security</code>来获得强度较弱的HTTP Basic认证外，我们还可以修改<code>management.port</code>和<code>management.context-path</code>来提高攻击成本. 是的，我对每一个服务都修改了以上两个属性，并且兼容了Eureka Server，Hystrix Dashboard，Spring Boot Admin，使这些监控服务仍能正确工作. 因为对以上两个参数修改，我们的监控路径有所变化，如下表：</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">module name</th>
      <th style="text-align: center">docker compose service name</th>
      <th style="text-align: center">application name</th>
      <th style="text-align: center">server port</th>
      <th style="text-align: center">management port</th>
      <th style="text-align: center">management context path</th>
      <th style="text-align: center">scalable</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">account-ms</td>
      <td style="text-align: center">account</td>
      <td style="text-align: center">account</td>
      <td style="text-align: center">10014</td>
      <td style="text-align: center">10248</td>
      <td style="text-align: center"><strong>/78d504ff-82e8-4a87-82e8-724d72d1171b</strong></td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: center">api-gateway-ms</td>
      <td style="text-align: center">gateway</td>
      <td style="text-align: center">gateway</td>
      <td style="text-align: center">7291</td>
      <td style="text-align: center">10211</td>
      <td style="text-align: center">/fb83deee-dd46-472b-99a9-f0ebffe20d0e</td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: center">config-ms</td>
      <td style="text-align: center">config_server</td>
      <td style="text-align: center">config-server</td>
      <td style="text-align: center">10888</td>
      <td style="text-align: center">10481</td>
      <td style="text-align: center">/f7597180-e480-400e-81a0-847c22e2e0b8</td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: center">eureka-registry-ms-1</td>
      <td style="text-align: center">eureka1</td>
      <td style="text-align: center">registry</td>
      <td style="text-align: center">8763</td>
      <td style="text-align: center">9274</td>
      <td style="text-align: center">/55395018-70b7-47c3-8fef-5bf24c9da9af</td>
      <td style="text-align: center">×</td>
    </tr>
    <tr>
      <td style="text-align: center">eureka-registry-ms-2</td>
      <td style="text-align: center">eureka2</td>
      <td style="text-align: center">registry</td>
      <td style="text-align: center">8762</td>
      <td style="text-align: center">10177</td>
      <td style="text-align: center">/e5da837b-a575-4447-b037-100850226a11</td>
      <td style="text-align: center">×</td>
    </tr>
    <tr>
      <td style="text-align: center">hystrix-dashboard-ms</td>
      <td style="text-align: center">hystrix_dashboard</td>
      <td style="text-align: center">hystrix</td>
      <td style="text-align: center">8193</td>
      <td style="text-align: center">7104</td>
      <td style="text-align: center">/9511d89d-6488-4293-8df8-c4feb8681e83</td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: center">membership-ms</td>
      <td style="text-align: center">membership</td>
      <td style="text-align: center">membership</td>
      <td style="text-align: center">10673</td>
      <td style="text-align: center">10391</td>
      <td style="text-align: center">/a6da3b6f-4b59-11e7-9226-0242ac130004</td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: center">order-ms</td>
      <td style="text-align: center">order</td>
      <td style="text-align: center">order</td>
      <td style="text-align: center">8295</td>
      <td style="text-align: center">10848</td>
      <td style="text-align: center"><strong>/78d504ff-82e8-4a87-82e8-724d72d1171b</strong></td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: center">product-ms</td>
      <td style="text-align: center">product</td>
      <td style="text-align: center">product</td>
      <td style="text-align: center">8040</td>
      <td style="text-align: center">10912</td>
      <td style="text-align: center"><strong>/78d504ff-82e8-4a87-82e8-724d72d1171b</strong></td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: center">spring-boot-admin-ms</td>
      <td style="text-align: center">spring_boot_admin</td>
      <td style="text-align: center">spring-boot-admin</td>
      <td style="text-align: center">7020</td>
      <td style="text-align: center">9218</td>
      <td style="text-align: center">/e58a0ff5-9f60-4545-9aa2-2b91c8a6d53b</td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: center">tcc-coordinator-ms</td>
      <td style="text-align: center">tcc_coordinator</td>
      <td style="text-align: center">tcc</td>
      <td style="text-align: center">11020</td>
      <td style="text-align: center">12841</td>
      <td style="text-align: center"><strong>/78d504ff-82e8-4a87-82e8-724d72d1171b</strong></td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: center">zipkin-ms</td>
      <td style="text-align: center">zipkin_server</td>
      <td style="text-align: center">zipkin-server</td>
      <td style="text-align: center">9411</td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">×</td>
    </tr>
  </tbody>
</table>

<h2 id="结语">结语</h2>


	  ]]></description>
	</item>


</channel>
</rss>
